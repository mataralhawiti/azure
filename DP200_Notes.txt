#1 : elastic pool replication

create azure sql elastic pool in the other region
Not
Enable geo-replication for each database !


@@ must review storage account comparion table
#2 : redundecy strategy for storage ( outage in one Azure Avialibty Zone )
Data redundecy strategy -> ZRS
Storage account type -> Gen 2 cuz it supports ZRS , lower cost 


Not
Data redundecy strategy -> GRS
    : will replicate to another Azure Region Not Azure Avialibty Zone
    : Also, it increases the cost. and the requirements only asked for ZRS

Storage account type -> Block Blob storage
    : spcialized account used to store Block Blobs and append Blobs
    : lower latency and higher transactions rates
    : only supports premium, which is expensive



#3 : daily adminstrative tasks for elastic pool
elastic databases Jobs

NOT
SQL Agent -> for on-premise or managed instance


# 4 : granular access (read and write) to CosmosDB for external app that we can't trust with master keys.
Credential key -> Resource Token
Permission -> All

Not 

Credential key -> Azure AD
    : azure AD user is used to read data in CosmosDB from azure portal or Data explorer 
    : you can't use Azure Ad user to authenticate via API


#5 :
    use Shared Access Signature (SAS) because it gives granular access control to specific blob and with expiration date

#6 :
    Always Encrypted with a randomized type won't allow for JOINS or GROUPS and etc.
    randomized type will encrype columns with random generated values.

    We should use Always Encrypted with a deterministic type, which will generate same values


# Dynamic Data Masking (DDM) :
    : with (random, partial, or other functions) is used for limit data exposure NOT Data Encryption
        -> Azure SQL : can be done via portal
        -> DWH : only powershell or API

    : Dynamic data masking policy
        -> SQL users excluded from masking 
        -> Masking rules
        -> Masking functions 
            -> Default	: Full masking according to the data types of the designated fields
                ADD MASKED WITH (FUNCTION = 'default()')
            -> Email: Masking method that exposes the first letter of an email address and the constant suffix ".com"
            -> Random : A random masking function for use on any numeric type to mask the original value with a random value within a specified range.
            -> Custom String (partial): Masking method that exposes the first and last letters and adds a custom padding string in the middle. prefix,[padding],suffix
                definition syntax: 
                    FirstName varchar(100) MASKED WITH (FUNCTION = 'partial(prefix,[padding],suffix)')
                alter syntax: 
                    ALTER COLUMN [Phone Number] ADD MASKED WITH (FUNCTION = 'partial(1,"XXXXXXX",0)')
                    ALTER COLUMN [Phone Number] ADD MASKED WITH (FUNCTION = 'partial(5,"XXXXXXX",0)')
                    ALTER COLUMN [Social Security Number] ADD MASKED WITH (FUNCTION = 'partial(0,"XXX-XX-",4)')


# Azure Stream Anyltics ASA
requirements :
    group events per line/product within specific time interval
    not exceeding max duration for the window
    filter out periods of time with no events getting
    count each event once

    Correct Answer --> Session window

    Not Correct Answer --> Tumbling window
        : fixed-size, non-overlapping and continuse time interval, eahc event occurs once
        : HOWEVER => 
            doesn't check the time duration between events
            doesn't filter out periods of time when no events are streamed


    Not Correct Answer --> Hopping window
        : fixed-size, continuse time interval.
        : if hop size is less than Window size, hopping windows overlap
        : doesn't check the time duration between events
        : doesn't filter out periods of time when no events are streamed


    Not Correct Answer --> Sliding window
        : fixed-size, continuse time interval.
        : produce output only when events occur, 
        : you can filter out periods of time when no events are streamed
        : HOWEVER => 
            Windows may overlap 
           doesn't check the time duration between events

    : use Reference Input for data that never or rarely change


# Query Performance Insight
    : allows to view db queries that consume most resources, and those take long time to run
    : NOT -> create or drop index


# SQL Database Advisor 
    : it allows to view recommondations for creating and dropping indexes
    : fixing schemas
    : paramtrize queries


# Azure Advisor
    : provides recommondations for  Avialibty, security, Performance and cost
    : it integrates with SQL Database Advisor to provide recommondations for creating and dropping indexes


# IoT solution
    1. create blob container
    2. create Azure Stream Anyltics job with edge hosting
    3. configure azure blob storage container as save location for [job definition]
    4. set up an IoT Edge environment on IoT device, and add stream Anyltics module
    5. Configure routes in IoT Edge

    : IoT Edge is a solution that analyze data on devices instead of the cloud, which will help reducing latency and bandiwth
    : routes in IoT Edge, this will upstream events from Stream Anyltics job to an IoT Hub in the cloud, allowing Azure functions
        for example to process data

    : you shouldn't configure Stream Units (SU), because Stream Anyltics Job with Edge hosting 
        doesn't consume SU. it only needed with cloud hosting


# PolyBase in DWH
    : External Data Source
    : External File Formate
    : External Table


# DW resource classes (resources and concurrency)
    : Smaller resource classes reduce the maximum memory per query, but increase concurrency.
    : Larger resource classes increase the maximum memory per query, but reduce concurrency.

    : Resource classes use [concurrency slots] to measure resource consumption

    : Type :
        -> Static resources classes ==> for increased concurrency on a data set size that is fixed.
                                    ==> allocate same amount of memory regardless of the current performance level
                                    ==> staticrc10, staticrc20, staticrc30, staticrc40, staticrc50, staticrc60, staticrc70, staticrc80

        -> Dynamic resource classes ==> for data sets that are growing in size and need increased performance as the service level is scaled up.
                                    ==> allocate a variable amount of memory depending on the current service level
                                    ==> smallrc, mediumrc, largerc, xlargerc

    : By default, each user is a member of the dynamic resource class [smallrc]

    : Concurrency slots
        -> Concurrency slots are a convenient way to track the resources available for query execution. 
        -> They are like tickets that you purchase to reserve seats at a concert because seating is limited. 
        -> The total number of concurrency slots per data warehouse is determined by the service level
        -> Example :
            : If each query requires 10 concurrency slots and there are 40 concurrency slots, then only 4 queries can run concurrently.
    
    :Resource class precedence
        : Dynamic resource classes take precedence over static resource classes. 
            -> For example : if a user is a member of both mediumrc(dynamic) and staticrc80 (static), queries run with mediumrc.
        : Larger resource classes take precedence over smaller resource classes.
            -> For example, if a user is a member of mediumrc and largerc, queries run with largerc. Likewise, if a user is a member of both staticrc20 and statirc80, queries run with staticrc80 resource allocations.

    : recommondations
        -> Choose a dynamic resource class when queries are complex, but don't need high concurrency
        -> Choose a static resource class when resource expectations vary throughout the day



# HDInsight Monitoring 
    : Apache Ambari -> to access YARN Queue Manager to monitor job distribution across the queue.
    : Apache Ambari -> is enabled spreatly for each HDInsight cluster. Not consolidated view for all clusters.
                       only support e-mail and SNMP alerts
    : Azure Monitor Logs -> to get consolidated view across all HDInsight clusters in scope. For exmaple, CPU, RAM and Storage.
                            Metrics and Logs from multiple HDInsight clusters can be sent to [Azure Anyltics workspace] in Azure Monitor


# DMVs
    : sys.dm_pdw_exec_requests -> for running or recently ran queries
    : sys.dm_pdw_sql_requests -> distribution run times for a specific query step


# Core SQL API
    : support schema less data store
    : semi-structure data
    : allows to use SQL-like queries

    Not
    : Gremlin API -> doesn't have SQL-like queries
    : Table API -> querying data using OData and Lanaguge Integrated Query (LINQ). No SQL-like queries
    : MongoDB API -> No SQL-like queries


# CosmosDB Consistency Level explained
    : Latency, Availability, latency and Throughput Trade off
    : we can override per-request Consistency

    : Strong => Bounded-stalenss => Session => Consistent Prefix => Eventual
    : going from left to right -> low latency, higher availability, better read scalability

    : Strong Consistency 
        -> once operation is complete, it's guaranteed that it will be to all subsequent requests in db
        -> we call it perfect Consistency as well

    : Bounded stalenss
        -> offer [ Consistent Prefix ]
            -> Consistent Prefix => means replication will always happen in order of those writes. you won't ever see out-of-order records
        -> it guarantees that you get perfect Consistency (Strong Consistency) outside of a stalenss window, but within that stalenss window you're gonna see only Consistent Prefix guarantees.
        -> stalenss Window is defined by time interval and number of operations

    : Session Consistency
        -> guarantees Strong Consistency within the session 
        -> this way you don't pay for perforamcne plenty of getting globaly strong Consistency
        -> within that session you'll be able to read your writes, 
        -> very good throughput and latency
    
    : Consistent Prefix
        -> the only guarantee that you'll never see out-of-order writes or gaps in time 

    : Eventual Consistency
        -> weakest form of Consistency
        -> the only guarantee is the the replicas will eventually convverge 
        -> lowest cost for read out of all other levels
            -> meaning : the read is always be served out of a single replica , it doesn't need to achive quorum, and it always will have lowest latency because you can read from nearest replica.


    : We can override/set consistency level in Connection time or During Request

# CosmosDB Consistency Level example
    : Scenario as :
        : Employee A, B, C 
        : Employee A, changed column "Status" from "Pending" to "Closed"
        : Strong Consistency Level

        : what's all three Employee can read before it's [!! committed !!] and before [!! Synchr !!] occurs?
            All Employee will read "Pending" even Employee A until the item is committed and Synced


# CosmosDB Consistency Level 
    : Scenario as :
        : Employee A, B, C 
        : Employee A, changed column "Status" from "Pending" to "Closed"
        : Strong Consistency Level

        : what's all three Employee can read before it's [!! committed !!] and before [!! Synchr !!] occurs?
            All Employee will read "Pending" even Employee A until the item is committed and Synced

    
    : Scenario as :
        : Employee A, B, C 
        : Employee A, changed column "Status" from "Pending" to "Closed"
        : Session Consistency Level

        : what's all three Employee can read before data is replicated [!! Synch !!] occurs?
            Employee A -> "Closed"
            Employee B, C -> "Pending"

        : With Session Consistency 
            ==> same user is guaranteed to read the same value within a single Session, Even before [replication occurs] !!!! 
            ==> the use writes the data can read the same value
            ==> users outside the seesion (Employee B&C) won't read the data because the data has not replicated yet

    
    : Scenario as :
        : data store for multi region application, choose Consistency levle with lowest latency between app and CosmosDB

        -> Eventual Consistency
            -> as little latency as possbile, without guarantee of reading operations using the latest committed write. 



# CosmosDB Request Unit (RU):
    : Scenario as
        : Throughput provisoned in container level, and -> under-provisioned Request Unit

    : Mertics
        -> [Total Request Unites by Container]
            -> it indicates how many RU were consumed.

    
    : Not 
        -> [Request count by HTTP status code]
            -> 429 http status code => indicates the container exceeded the configured throughput.

        -> [Max consumed RU per second per partioin]
            -> use this mertic to identify which partioin is consuming the most RU provisoned for given container or DB

        -> [provisioned throughput by container]
            -> it's used to monitor how much throughput provisioned by container. HOWEVER, you can't determine how much throughout is consumed.






# Azure SQL DW Gen2 Cashing 
    : Scenario as :wokring data set doesn't fit compelelty into the cache. 
        -> Configure Monitor Alert with low [cache hit percentage] and high [cache used percentage] Metrics

    : Not 
        -> high [cache hit percentage]  because cacheing with a high hit percentage donates a good use of cacheing.

    : Not 
        -> low [cache used percentage] because that means you have plenty of space in your cache


# Monitor Files uploaded to Data Lake are larger than 20 MB in HDInsight
    : Metrics -> Blob Capacity Avergae
    : Dimension -> BlobTier

    : We need [BlobTier] Dimension to filter out already processed data because Files are moved
        to cool tier after HDInsight process them.

    : Not
        -> [Used Capacity Avergae] Metrics, because it shows the Avergae Capacity for Azure account storage.

    : Not 
        -> [BlobType] Dimension
        -> [ResponseType] Dimension


# Monitor chart to show how much data uploaded in each data fram. it's being uploaded to Blob storage
    : Metrics Namespace -> Blob
    : Metrics -> Ingress
    : Aggregation (Dimension) -> Sum


    : Not
        -> Metrics Namespace -> account/files/queue cuz our requirements say Blob container
        -> Metrics -> transactions cuz it only shows transactions count not type or size


# Dynamic Data Maskig (DDM) for credit card. onyl 2 first digits and last 4 digits can be visiible
    : use partial function to mask. it recives 3 arguments
        -> Query 
        ALTER TABLE [PAYMENT_CREDIT_CARD]
        ALTER COLUMN [CREDIT_CARD]
        ADD MASKED WITH (FUNCTION = 'partial(2, "xx-xxxx-xxxx-", 4')

    : arg1 -> prefix of how many chars should be showen from the beginging of the calue
    : arg2 -> pattern used for masking 
    : arg3 -> suffix of how many chars are shown from the end.  


# Query encryoed columns (Always Encrypted) 
    : ALTER ANY COLUMN MASTER KEY 
        -> (Required to manage [create and delete] master key, which encrypes/decrypts column encryption keys )

    : ALTER ANY COLUMN ENCRYPTION KEY 
        -> (Required to mange [create and delete] a column encryption key.)

    : VIEW ANY COLUMN MASTER KEY DEFINITION 
        -> (Required to access and read the metadata of the column master keys to manage keys or query encrypted columns.)

    : VIEW ANY COLUMN ENCRYPTION KEY DEFINITION 
        -> (Required to access and read the metadata of the column encryption key to manage keys or query encrypted columns.)



# CosmosDB TTL 
    : Scenario
        single container, that store qulity checks (sucessful checks and defect detections).
        We found out no benefits of keeping sucessful cheeks, so we want to automatically elete those records after an hour.
        and defect detections records must be kept indefinitly.

    : we need to set up in 2 places 
        -> Set TTL in the container to (3600) .. seconds
        -> Programmatily set TTL of [defect detections records] to (-1)

    : This setting will keep all items in the container for an hour, expect defect detections records
    
    : Not
        -> Set TTL in the container to (-1) because this means never expire by default
        -> Programmatily set TTL of [defect detections records] to (null) because that means inhert TTL settings from uuper container.

# Azure SQL Database Auditing
    : Configure Log Analytics as audit log distention
    : Enable Audit log at the server Level. Because this will enable auditing on all dbs in the server

    : Not
        -> Not DB auditing Level
        -> Not Blob storage as a distention becuase you will then need to consume these logs with other services like Databricks and HDInsight


# Azure Stream Anyltics Job Optimization
: Scenario 
    SA job uses 18 SU, ASA mertics and SU percentage utlization usage is 90% from last month
    you need to optimize ASA job 

: we should 
    -> Partion Data for query paralleization
        : partioning data increases job throughput and allows for an optimized use of avialble SU

    -> increase SU count for the job
        : SU represents compute the compute resources allocated for the job
        : Too high SU % utlization indicates a high memeory consumption and higher latency to process events
        : it's recommened to keep SU % below 80%

    -> Query paralleization + high SU count = optimized job Performance



# SQL DWH distribution
    : Replicated Tables for Dims table
        -> for small Dimension tables that change infrequnetly.
        -> By replicating tables across Compute Nodes we elimniate the need for data transfer during JOIN with Fact table
    
    : Hash distribution
        -> use Hash distribution for Fact table.
        -> Hash distribution copies rows across Compute Nodes by using hashing function against column.
        -> Rows with columns that have the same hash value are copied to the same compute Node.



# ASA Window functions
    : Scenario
        capture events that repeat and do not overlap. you also want to capture time periods when there is no events !    
    : -> use Tumbling Window
    : Tumbling Window -> it segments event data into distinct time segements and then perform a function against each segement. it allows events repeat but not overlapping

    : Not
        -> Hopping Window, similar to umbling() but it allows overlapping
        -> Sliding Window, it produces output only when event occurs
        -> Session Window, it groups events that arrive at similair times. it filters out periods when there is no data.

    : Functions comparion
        -> Tumbling :
            -> None overlapping
            -> events belong to only one window
            -> fixed-szie time, and continuse

        -> Hopping :
            -> overlapping
            -> fixed-szie time, and continuse
            -> Hopsize : how long a window will overlap with previous Window --> this could result in events belong to one or more than one window

        -> Sliding :
            -> overlapping
            -> fixed-szie time, and continuse
            -> it's similair to setting hopping Window function with a hop size equal to zero, the only Exception here is Sliding Function only produce an output when an event occurs

        -> Session :
            -> Group events that arraive at simnilar time.
            -> events can belong to more than one window
            -> have a variable length


# Run Analytics on data to automatically generate bar charts, and it has to be Scheduled
    : Scheduled Databricks Job
    : Not
        -> Scheduled Runbook in azure automation, becuase it's not generate visual !
        -> Scheduled Azure function, becuase it's not generate visual!.
        -> Scheduled WebJob, becuase it's not generate visual! ** WebJob runs in context of aZURE Service App



# Query Store 
    : Query store can help with things like 
        -> "Creating x number of indexes on able"
        -> Increase VM size. For example, if our SQL on VM

    : Query Store allows to compare Performance before and after an anticopated change.
    : Query Store can only monitor one database.
        -> ALTER DATABASE XXXX SET QUERY_STORE = ON


# Azure Table [row key] and [partioning key]
    : partioning key allows to create logical partions that will enhance the Performance
    : row key, uniquely identifies the row


# HDInsight perofrmance optimization with mini cost.
    : Scenario
        logs files for each server stored in Data Lake Gen2, inside a single folder. and data is analyzed in HDInsight
    : Combine Daily log files into one file.
        -> HDInsight and other Analytics enegin have per-file overhead during processing.
        -> Using small fils will quickly utlize the avialble throughput for Azure Data Lake Gen2 to read data
        -> So, Organizing your data in larger files with szie from 265MBto 100GB result in better performance.

    : Separate the log files into a daily generated folder.
        -> partioning files in time series folders help HDInsightto load only subset of data thus imporve performance.
        -> we can use hierarchic structure like \dataset\yyyy\mm\dd\datafile_yyyy_mm_dd.tsv

    : Not
        -> increase number of worker nodes, it enhance HDinsight throughput, but it increases the cost and don't fix high utlization of ADL throughput
        -> Cool Tier for ADL Gen2, because this tier is optimzied for data that infrequnetly accessed. If we do so in this Scenario, we'll add more data access and transfer cost as we access the log data daily frequnetly


# Elastic Pool with (DTU) provisioning model.
    : Scenario
        Monitor database based on a mertic that best anticopate out-of-Capacity and perofrmance issues.

    : DTU percentage mertic
        -> represents the percentage of DTU that is consumed by database, independent of the number of provisioned DTUs.
        -> if you increase DUT of database, you don't need to edit alert for new Capacity

    : Not
        -> used DTU mertic
            -> it could be used to anticopate perofrmance issues when DTU consumption is close to limit. 
            -> you need to edit this alert if you increase DTU

        -> CPU percentage mertic, Data IO percentage mertic
            -> For DTU based models, CPU, Data IO and storage are bundled together in simplified performance mertic



# Sending Databricks application mertics to Azure Monitor
    : Dropwizard, mertics library
    : Not
        -> Log4j : sending application logs !!! not mertics


# Azure CosmosDB partioning key
    : Scenario
        App spread across 5 regions, and oringe region is an item in CosmoseDB document, and we have sensorID (unique).
        Queries originated from app usuallt filter the result by region and sensorID 
    
    : Region with pre-calculated stuffix based on sensorID
        -> this partioin key will distribute all the documents evenly across logical partions.
        -> Inclduing pre-calculated suffix based on known value (sensorID), will greatly imrpove both write and read throughput across partitions.

    : Not
        -> Region, this field only contains 5 values ! resulting in a small number of logical partioins and a low read and write throughput
        -> sensorID, it's unique which will result in millions of logical partioins, impacting read and write throughput
        -> timestampe with random suffix,
            -> this partion key will distribute all documents across logical partions . 
            -> greatly increase write throughput
            -> HOWEVER, reading specific item will become harder because we don't know which suffix was used, imapcting read throughput



# Azure Data Lake Gen2, ACL in POSIX-compatible format.
    : Scenario
        Alan and Kerry are memebers of Marketing Azure AD secuirty group, set as a primary one for Azure AD accounts.
        Kerry and David are memebers of Finance secuirty group, set as a primary for Davids' account

        You set up a new directry in ADL Gen2, and owner group as Finance.
        Kerry creates a new text file in the directry.
        Your audit report indicates that access control list (ACL), for that file set to 640 in POSIX format.

        You need to determine what access Permission Alan, kerry and David have to the new file.

    : Access will be as :
        -> Alan : No Access
        -> Kerry : Read + write
        -> David : Read only

    : File Permission consists of 3 digits :
        -> Owner : the user who created the item automatically become an Owner
        -> Owner Group : in ADL Gen2, owner group is inherited from the parent folder.
        -> Everyone : 
    
    : ADL Gen2, supports ACL in POSIX format-compatible format. assging numbers to different Permission Combinations. For example:
        -> Read Only : 4
        -> Write Only : 2
        -> Exceute Only : 1
        -> No Access : 0
        -> Read + Write : 4+3 = 6
        -> Read + Execute : 4+1 = 5
        
    : In our Scenario, 640 Permission set on the file can be translated :
        -> Owner (Kerry) - [First digit] : 6, which means READ + WRITE (6)
        -> Owner Group (Finance, which includes David) - [Second digit] : 4, which means READ only () 
        -> Everyone - (Alan) [third digit]: 0, which means No access
    : hierarchical Namespace

# Configure PolyBase to load data from ADL Gen2 to Azure DW, and shoudl NOT use AZure AD service principle.
    1. create aa scoped Credential with the azure storage access key
    2. Create an external data source with HADOOP type.
    3. Create an external file format
    4. create an external table.
    5. load the data inot the table

    : ADL Gen2 use supports the use storage access keys for PolyBase access
    
    : Not 
        ->  "create aa scoped Credential with client id and OAuth 2.0 token endpoint"
            -> you can use this option to connect to ADL Gen2, but it requires azure AD Service principle
    
        -> "BLOB_STORAGE" type for external data source
            -> this type is using when executing bulk operations with on-premises SQL Server or Azure SQL Database.


# Import Data into SQL DWH from Azure Storge Blob container
    : Extenal Data Source with HADOOP type.
    : We must use TYPE=HADOOP, when our data source is ADL or Storage Blob container
        ->      TYPE = HADOOP
                LOCATION ='wasbs://xxx@sampl.blob.core.windows.net' --> Blob Storage Endpoint, ADL Gen2 has different Endpoint structure.

        ->      TYPE = HADOOP
                LOCATION = 'abfs[s]://file_system@account_name.dfs.core.windows.net/<path>/<path>/<file_name>' --> ADL Endpoint structure
    : Not 
        -> "BLOB_STORAGE" doesn't represent Azure Blob Storag.
            -> it simply designates the data source as one that will be used with BULK IMPORT or OPENROWSET
            -> it can't be used with external data sources such as Blob storage 
            -> Extenal data source : is one that isn't located in Azure SQL Database.


# Azure SQL Database, Monitor performance by capturing a history of query plan changes over time.
    : ALTER DATABASE XXX SET QUERY_STORE = ON (OPERATION_MODE = READ_WRITE) ... it's disabled by default
    : Not
        -> SQL Server profiler
            -> traces SQL-related events, such as query executions and logins
            

# Choose the right integration and automation services in Azure
    : Microsoft Flow
    : Azure Logic Apps
    : Azure Functions
    : Azure App Service WebJobs


# Access Stroge Blob from Databricks using KeyVault
    : Secret Scopes
        -> Azure Key Vault-backed scopes
        -> Databricks-backed
    # Databricks notebook source
    dbutils.fs.mount(
    source = "wasbs://mycontainer@brciksstore.blob.core.windows.net",
    mount_point  = "/mnt/blobstoremnt",
    extra_configs  = {"fs.azure.account.key.brciksstore.blob.core.windows.net":dbutils.secrets.get(scope = "SecretScopeBlob", key = "dbkStorageKey")})



# Implement Azure Databricks with a Cosmos DB endpoint



# Azure Monitor
    : metrics
        -> are numerical values that describe some aspect of a system at a particular point in tim
        -> tell you how the resource is performing and the resources that it's consuming.

        -> Numerical values only ***
        -> Collected at regular intervals ***
        -> View in Azure portal => Metrics Explorer

    : Logs
        -> contain different kinds of data organized into records with different sets of properties for each type. 
        -> Telemetry such as events and traces are stored as logs in addition to performance data so that it can all be combined for analysis

        -> Text or numeric data ***
        -> May be collected sporadically as events trigger a record to be created.
        -> View in Azure portal => Log Analytics

        :  Log Analytics
            -> to queries the logs

    : Collect monitoring data
        : Different sources of data for Azure Monitor will write to either a 
            -> Log Analytics workspace (Logs) 
            or 
            -> the Azure Monitor metrics database (Metrics) 
            or 
            -> both

    : diagnostics
        -> This will collect telemetry for the internal operation of the resource.

    : Monitoring solutions

        : Application Insights 
            -> monitors the availability, performance, and usage of your web applications whether they're hosted in the cloud or on-premises.

        : Azure Monitor for containers
        : Azure Monitor for VMs
    
    : Alerts in Azure Monitor
        -> Alert rules in Azure Monitor use action groups

# Storage metrics in Azure Monitor
    : metrics

        : Capacity metrics
            : Account Level
                -> UsedCapacity
            : Service Level
                : Blob storage
                    -> BlobCapacity
                    -> BlobCount
                    -> ContainerCount
                    -> IndexCapacity (ADL Gen2)
                : Table storage
                    -> TableCapacity
                    ->
                    ->
                : Queue storage
                    -> 
                    -> 
                    -> 
                : File storage

        : Transaction metrics [available at both account and service level]
            -> Transactions
            -> Ingress
            -> Egress
            -> SuccessServerLatency
            -> SuccessE2ELatency
            -> Availability
    
    : Metrics dimensions
        -> BlobType
        -> BlobTier
        -> GeoType
        -> ResponseType
        -> ApiName
        -> Authentication



# Understanding block blobs, append blobs, and page blobs
    : Type 
        -> Block
        -> appeand
        -> page
    : Blob lease 
        -> lease id or its blocks



# Streaming Unit (SU)
    : Streaming Units (SUs) represents the computing resources that are allocated to execute a Stream Analytics job. 
    : The higher the number of SUs, the more CPU and memory resources are allocated for your job

    : To achieve low latency stream processing, Azure Stream Analytics jobs perform all processing in memory

    : SU % utilization metric
        -> which ranges from 0% to 100%
        -> describes the memory consumption of your workload.

        -> For a streaming job with minimal footprint, this metric is usually between 10% to 20%. 
        -> If SU% utilization is low and input events get backlogged, your workload likely requires more compute resources, which requires you to increase the number of SUs. 
        -> It’s best to keep the SU metric below 80% to account for occasional spikes.
        -> Microsoft recommends setting an alert on 80% SU Utilization metric to prevent resource exhaustion

        -> 80% utlization probaly means job will fail
        -> use it with MAx Aggregation to get used capacity
        -> Increasing number of streaming units for a job might not reduce SU% Utilization if your query is not fully parallel.

        ->  PARTITION BY


# SQL Server Encryption 
    : Encryption Mechanisms
        -> Transact-SQL functions
        -> Asymmetric keys
        -> Symmetric keys
        -> Certificates
        -> Transparent Data Encryption

    : Transparent Data Encryption (TDE)
        === use ===> symmetric key called the Database Encryption Key (DEK)
                    === protected by ===> Data encryption protector (TDE protector)
                                        === either  === > service-managed certificate in the master database of the server 
                                                    === > or  
                                                    ===> asymmetric key protected by an EKM module or Azure Key Vault

    : Always Encrypted
        ===> Column Encryption keys (CEK) : to encrypt data in the database, and they're stored in the database in the encrypted form
        ===> Column Master keys (CMK) :  to encrypt encryption keys, are stored in an external key store

        : supports two types of encryption: 
            -> randomized encryption
                : always generates the same encrypted value for any given plain text value. 
                : Using deterministic encryption allows point lookups, equality joins, grouping and indexing on encrypted columns.
                : it may also allow unauthorized users to guess information about encrypted values by examining patterns in the encrypted column

            -> deterministic encryption.
                : uses a method that encrypts data in a less predictable manner
                : Randomized encryption is more secure, but prevents searching, grouping, indexing, and joining on encrypted columns.





    : TDE is the recommended choice for encryption at rest, and we recommend TLS for protecting data in-transit
    : Recommendation :
        ==> TDE to encrypt the entire database at rest
            ==> TLS to protect all traffic to the database.
                ==> Always Encrypted to protect highly sensitive data from high-privilege users and malware.



# Row-Level Security
    : query 
        CREATE SECURITY POLICY SalesFilter  
        ADD FILTER PREDICATE Security.fn_securitypredicate(SalesRep)
        ON dbo.Sales  
        WITH (STATE = ON);  
    
    : parts :
        -> SECURITY POLICY
        -> FILTER PREDICATE ==> [fn_securitypredicate(SalesRep)]   function name and its input
        -> ON ==> table name


# Re-encrypt Transparent Data Encryption (TDE), using KeyVault, powershell
    : Assign AAD to Azure SQL
    : Grant KeyVault Permission to Azure SQL 
    : Add KeyVault to Azure SQL, and set it as TED protector
    : Turn on TED 

    :-> if you do this via the portal, you DO NOT Need Step 1 & 2 :
        you get an AAD identity assgined and relevant Permissions set to Azure SQL via Azure KeyVault access policy automatically.



# Encrypt Transparent Data Encryption (TDE), using KeyVault, in geo-replicate in 2 regions :
    : start with secondary region.
    : On secondary, assgin KeyVault that in the same region
    : on secondary, assgin TDE protector

    : do the same in primary region 



# Easily identify queries within DMV in SQL :
    : use LABLE option

#  Azure Databricks Monitoring
    : default format for mertics data used by Azure Databricks => Ganglia
    : supports sending logs to Azure monitor => via 3rd party library
    : Mertics transfoned to Azure will be stored in : Azure Log Analytics workspace


# In-Memeory storage (In-Memeory OLTP) utlization
    : SELECT xtp_storage_percentage from sys.dm_db_resource_stats 
    Or
    : DB > Monitor > mertics blade > in-memory OLTP storage percentage mertic 


# Azure Event Hubs
    : allows you to capture, retain ,and replay telemetry data.
    : it accepts data stream over HTTP and AMQP

# Azure IoT Hub
    : it accepts data stream over HTTP, AMQP, MQTT

# Azure Event Gird
    : publish-subscribe platform for events
    : Events publisher, sends events to Event Grid. And Subscriber subsbcribe to the events they want to handle.
# Azure Relay
    : Allows client apps to connect to services hosted on a private networkr over internet
    : allows client apps to access on-premise services through Azure
    : it (Azure Functions as well) doesn't accepts messages over AMQP


# Dynamic Data Maskig (DDM) vs Row Level Security (RLS)
    : DDM -> allows limiting the exposure of specific fields
    : RLS -> Hide the whole row not just fields/columns

# DWH re-partioin
    : Disable the columnstore index 
    : user ALTER TABLE statement with SPLIT clause
    : Rebuild the columnstore index

# Azure IT Service Management Connector (ITSMC)

# Azure Data Factory pipeline Monitoring 
    : To store ADF pipeline run history more than the default (45) we can send logs to Azure Monitor by configuring Diagnostics logs to send data to Blob Storag

# give an existing user Admin rights to DB
    : ALTER ROLE db_owner ADD MEMEBER matar
    : Not
        - ALTER USER 

# TDE with customer-manged encryption key
    -> Assuming : Azure AD identity is already assgined in Azure SQL DB Server
    1. Create Azure KeyVault and generate new key
    2. Grant KeyVault Permissions (getm weapKey, UnwrapKey) to that Azure AD identity assgined to Azure SQL Server
    3. Add KeyVault Key to Azure SQL Server
    4. Set TDE protector to use the KeyVault key
    5. Enable encryption in DB

    : Not
        -> Create MAster key in master DB,  create server certificate usin master key, create encryption ket from that certificate
            => this setup for configuring TDE for  On-premise SQL 
    : By Default, TDE is enabled in new provisoned Azure SQL using services-manged encryption key


# Azure Data Factory
    : Scenario
        connect to on-premise db and move data periodically to Azure SQL DB, the pipeline is allowed to run at specific, fixed-size time interval
    : Component -> Linked Service
    : Execution -> Tumbling Window in Pipeline execution trigger 

# Azure CosmosDB CLI 
    : az [cosmosdb] => to create account
    : az [cosmosdb creat] => to creat DB
    : --kind GlobalDocumentDB
    : --default-consistency-level Strong
    : --enable-multiple-write-locations => provision CosmosDB in multi-regions with multiple write regions
    : --enable-automatic-failover => if you want to enable automatic failover in case of Disatir

# Watermark approach in Azure Data Factory
    1. lookup the value from Watermark table
    2. lookup the value from customer table
    3. copy delta using Watermark
    4. execute SP to update Watermark table

# Log API calls to ADL Gen2
    : Scenario
        logs should be stored in BLOB Storag
        only include operations about API calls 
    : From Diagnostics Setting blad config 
        -> LOG section enable ==> Requests ==> capture every API request made to the account
        -> LOG section enable ==> Audit ==> to capture breakdown of the operation made to APIs
        
# SQL Secuirty policy Predicate 
    CREATE SECURITY POLICY SalesFilter_ext
    ADD FILTER PREDICATE Security.fn_securitypredicate(SalesRep)
    ON dbo.Sales_ext  
    WITH (STATE = ON);

# Configure number of DTU for multiple DBs in Elastic pool
    : determine number of total DTUs that are used by all DBs combined 

# Access ADL Gen2 from Azure Databricks
    1. Create Databricks secret scope in the workspace
    2. Add ADL Gen2 account key and a service principle seconds to that scope
    3. Configure the storage account key wwith spark.conf.set().
        => to ensure that storage account reads the key from secret scope with dbutils.secrets.get() method
    4. mount a filesystem using a service principle
        => to securly mount the filesystem using service principle store in the secret scope.


    : Not 
        -> spark.sparkContext.hadoopConfigration()
            => you can use this to read directly from ADL Gen2, HOWEVER; storage account key will be exposed to all users who access the cluster

        -> Read from storage account using RDD API
            => this will requires you to use spark.sparkContext.hadoopConfigration()

        -> You can't use SAS with ADL Gen2

# Azure messaging services
    : Event Grid
    : Event Hubs
    : Service Bus