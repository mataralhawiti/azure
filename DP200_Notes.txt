#1 : elastic pool replication

create azure sql elastic pool in the other region
Not
Enable geo-replication for each database !


@@ must review storage account comparion table
#2 : redundecy strategy for storage ( outage in one Azure Avialibty Zone )
Data redundecy strategy -> ZRS
Storage account type -> Gen 2 cuz it supports ZRS , lower cost 


Not
Data redundecy strategy -> GRS
    : will replicate to another Azure Region Not Azure Avialibty Zone
    : Also, it increases the cost. and the requirements only asked for ZRS

Storage account type -> Block Blob storage
    : spcialized account used to store Block Blobs and append Blobs
    : lower latency and higher transactions rates
    : only supports premium, which is expensive



#3 : daily adminstrative tasks for elastic pool
elastic databases Jobs

NOT
SQL Agent -> for on-premise or managed instance


# 4 : granular access (read and write) to CosmosDB for external app that we can't trust with master keys.
Credential key -> Resource Token
Permission -> All

Not 

Credential key -> Azure AD
    : azure AD user is used to read data in CosmosDB from azure portal or Data explorer 
    : you can't use Azure Ad user to authenticate via API


#5 :
    use Shared Access Signature (SAS) because it gives granular access control to specific blob and with expiration date

#6 :
    Always Encrypted with a randomized type won't allow for JOINS or GROUPS and etc.
    randomized type will encrype columns with random generated values.

    We should use Always Encrypted with a deterministic type, which will generate same values


# Dynamic Data Masking (DDM) :
    : with (random, partial, or other functions) is used for limit data exposure NOT Data Encryption
        -> Azure SQL : can be done via portal
        -> DWH : only powershell or API

    : Dynamic data masking policy
        -> SQL users excluded from masking 
        -> Masking rules
        -> Masking functions 
            -> Default	: Full masking according to the data types of the designated fields
                ADD MASKED WITH (FUNCTION = 'default()')
            -> Email: Masking method that exposes the first letter of an email address and the constant suffix ".com"
            -> Random : A random masking function for use on any numeric type to mask the original value with a random value within a specified range.
            -> Custom String (partial): Masking method that exposes the first and last letters and adds a custom padding string in the middle. prefix,[padding],suffix
                definition syntax: 
                    FirstName varchar(100) MASKED WITH (FUNCTION = 'partial(prefix,[padding],suffix)')
                alter syntax: 
                    ALTER COLUMN [Phone Number] ADD MASKED WITH (FUNCTION = 'partial(1,"XXXXXXX",0)')
                    ALTER COLUMN [Phone Number] ADD MASKED WITH (FUNCTION = 'partial(5,"XXXXXXX",0)')
                    ALTER COLUMN [Social Security Number] ADD MASKED WITH (FUNCTION = 'partial(0,"XXX-XX-",4)')

            -> Credit Card : XXXX-XXXX-XXXX-1234
# Azure Stream Anyltics ASA
requirements :
    group events per line/product within specific time interval
    not exceeding max duration for the window
    filter out periods of time with no events getting
    count each event once

    Correct Answer --> Session window

    Not Correct Answer --> Tumbling window
        : fixed-size, non-overlapping and continuse time interval, eahc event occurs once
        : HOWEVER => 
            doesn't check the time duration between events
            doesn't filter out periods of time when no events are streamed


    Not Correct Answer --> Hopping window
        : fixed-size, continuse time interval.
        : if hop size is less than Window size, hopping windows overlap
        : doesn't check the time duration between events
        : doesn't filter out periods of time when no events are streamed


    Not Correct Answer --> Sliding window
        : fixed-size, continuse time interval.
        : produce output only when events occur, 
        : you can filter out periods of time when no events are streamed
        : HOWEVER => 
            Windows may overlap 
           doesn't check the time duration between events

    : use Reference Input for data that never or rarely change


# Query Performance Insight
    : allows to view db queries that consume most resources, and those take long time to run
    : NOT -> create or drop index


# SQL Database Advisor 
    : it allows to view recommondations for creating and dropping indexes
    : fixing schemas
    : paramtrize queries


# Azure Advisor
    : provides recommondations for  Avialibty, security, Performance and cost
    : it integrates with SQL Database Advisor to provide recommondations for creating and dropping indexes


# IoT solution
    1. create blob container
    2. create Azure Stream Anyltics job with edge hosting
    3. configure azure blob storage container as save location for [job definition]
    4. set up an IoT Edge environment on IoT device, and add stream Anyltics module
    5. Configure routes in IoT Edge

    : IoT Edge is a solution that analyze data on devices instead of the cloud, which will help reducing latency and bandiwth
    : routes in IoT Edge, this will upstream events from Stream Anyltics job to an IoT Hub in the cloud, allowing Azure functions
        for example to process data

    : you shouldn't configure Stream Units (SU), because Stream Anyltics Job with Edge hosting 
        doesn't consume SU. it only needed with cloud hosting


# PolyBase in DWH
    : Master Key
    : create a database-scoped credential => that secures the credentials to the blob storage
    : External Data Source
    : External File Formate
    : External Table

    : it's enabled by default in Azure DWH
    : you need to enable it for on-premise SQL sp_configure command


# DW resource classes (resources and concurrency)
    : Smaller resource classes reduce the maximum memory per query, but increase concurrency.
    : Larger resource classes increase the maximum memory per query, but reduce concurrency.

    : Resource classes use [concurrency slots] to measure resource consumption

    : Type :
        -> Static resources classes ==> for increased concurrency on a data set size that is fixed.
                                    ==> allocate same amount of memory regardless of the current performance level
                                    ==> staticrc10, staticrc20, staticrc30, staticrc40, staticrc50, staticrc60, staticrc70, staticrc80

        -> Dynamic resource classes ==> for data sets that are growing in size and need increased performance as the service level is scaled up.
                                    ==> allocate a variable amount of memory depending on the current service level
                                    ==> smallrc, mediumrc, largerc, xlargerc

    : By default, each user is a member of the dynamic resource class [smallrc]

    : Concurrency slots
        -> Concurrency slots are a convenient way to track the resources available for query execution. 
        -> They are like tickets that you purchase to reserve seats at a concert because seating is limited. 
        -> The total number of concurrency slots per data warehouse is determined by the service level
        -> Example :
            : If each query requires 10 concurrency slots and there are 40 concurrency slots, then only 4 queries can run concurrently.
    
    :Resource class precedence
        : Dynamic resource classes take precedence over static resource classes. 
            -> For example : if a user is a member of both mediumrc(dynamic) and staticrc80 (static), queries run with mediumrc.
        : Larger resource classes take precedence over smaller resource classes.
            -> For example, if a user is a member of mediumrc and largerc, queries run with largerc. Likewise, if a user is a member of both staticrc20 and statirc80, queries run with staticrc80 resource allocations.

    : recommondations
        -> Choose a dynamic resource class when queries are complex, but don't need high concurrency
        -> Choose a static resource class when resource expectations vary throughout the day



# HDInsight Monitoring 
    : Apache Ambari -> to access YARN Queue Manager to monitor job distribution across the queue.
    : Apache Ambari -> is enabled spreatly for each HDInsight cluster. Not consolidated view for all clusters.
                       only support e-mail and SNMP alerts
    : Azure Monitor Logs -> to get consolidated view across all HDInsight clusters in scope. For exmaple, CPU, RAM and Storage.
                            Metrics and Logs from multiple HDInsight clusters can be sent to [Azure Anyltics workspace] in Azure Monitor


# DMVs
    : sys.dm_pdw_exec_requests -> for running or recently ran queries
    : sys.dm_pdw_sql_requests -> distribution run times for a specific query step


# Core SQL API
    : support schema less data store
    : semi-structure data
    : allows to use SQL-like queries

    Not
    : Gremlin API -> doesn't have SQL-like queries
    : Table API -> querying data using OData and Lanaguge Integrated Query (LINQ). No SQL-like queries
    : MongoDB API -> No SQL-like queries


# CosmosDB Consistency Level explained
    : Latency, Availability, latency and Throughput Trade off
    : we can override per-request Consistency

    : Strong => Bounded-stalenss => Session => Consistent Prefix => Eventual
    : going from left to right -> low latency, higher availability, better read scalability

    : Strong Consistency 
        -> once operation is complete, it's guaranteed that it will be to all subsequent requests in db
        -> we call it perfect Consistency as well

    : Bounded stalenss
        -> offer [ Consistent Prefix ]
            -> Consistent Prefix => means replication will always happen in order of those writes. you won't ever see out-of-order records
        -> it guarantees that you get perfect Consistency (Strong Consistency) outside of a stalenss window, but within that stalenss window you're gonna see only Consistent Prefix guarantees.
        -> stalenss Window is defined by time interval and number of operations

    : Session Consistency
        -> guarantees Strong Consistency within the session 
        -> this way you don't pay for perforamcne plenty of getting globaly strong Consistency
        -> within that session you'll be able to read your writes, 
        -> very good throughput and latency
    
    : Consistent Prefix
        -> the only guarantee that you'll never see out-of-order writes or gaps in time 

    : Eventual Consistency
        -> weakest form of Consistency
        -> the only guarantee is the the replicas will eventually convverge 
        -> lowest cost for read out of all other levels
            -> meaning : the read is always be served out of a single replica , it doesn't need to achive quorum, and it always will have lowest latency because you can read from nearest replica.


    : We can override/set consistency level in Connection time or During Request

# CosmosDB Consistency Level example
    : Scenario as :
        : Employee A, B, C 
        : Employee A, changed column "Status" from "Pending" to "Closed"
        : Strong Consistency Level

        : what's all three Employee can read before it's [!! committed !!] and before [!! Synchr !!] occurs?
            All Employee will read "Pending" even Employee A until the item is committed and Synced


# CosmosDB Consistency Level 
    : Scenario as :
        : Employee A, B, C 
        : Employee A, changed column "Status" from "Pending" to "Closed"
        : Strong Consistency Level

        : what's all three Employee can read before it's [!! committed !!] and before [!! Synchr !!] occurs?
            All Employee will read "Pending" even Employee A until the item is committed and Synced

    
    : Scenario as :
        : Employee A, B, C 
        : Employee A, changed column "Status" from "Pending" to "Closed"
        : Session Consistency Level

        : what's all three Employee can read before data is replicated [!! Synch !!] occurs?
            Employee A -> "Closed"
            Employee B, C -> "Pending"

        : With Session Consistency 
            ==> same user is guaranteed to read the same value within a single Session, Even before [replication occurs] !!!! 
            ==> the use writes the data can read the same value
            ==> users outside the seesion (Employee B&C) won't read the data because the data has not replicated yet

    
    : Scenario as :
        : data store for multi region application, choose Consistency levle with lowest latency between app and CosmosDB

        -> Eventual Consistency
            -> as little latency as possbile, without guarantee of reading operations using the latest committed write. 



# CosmosDB Request Unit (RU):
    : Scenario as
        : Throughput provisoned in container level, and -> under-provisioned Request Unit

    : Mertics
        -> [Total Request Unites by Container]
            -> it indicates how many RU were consumed.

    
    : Not 
        -> [Request count by HTTP status code]
            -> 429 http status code => indicates the container exceeded the configured throughput.

        -> [Max consumed RU per second per partioin]
            -> use this mertic to identify which partioin is consuming the most RU provisoned for given container or DB

        -> [provisioned throughput by container]
            -> it's used to monitor how much throughput provisioned by container. HOWEVER, you can't determine how much throughout is consumed.






# Azure SQL DW Gen2 Cashing 
    : Scenario as :wokring data set doesn't fit compelelty into the cache. 
        -> Configure Monitor Alert with low [cache hit percentage] and high [cache used percentage] Metrics

    : Not 
        -> high [cache hit percentage]  because cacheing with a high hit percentage donates a good use of cacheing.

    : Not 
        -> low [cache used percentage] because that means you have plenty of space in your cache


# Monitor Files uploaded to Data Lake are larger than 20 MB in HDInsight
    : Metrics -> Blob Capacity Avergae
    : Dimension -> BlobTier

    : We need [BlobTier] Dimension to filter out already processed data because Files are moved
        to cool tier after HDInsight process them.

    : Not
        -> [Used Capacity Avergae] Metrics, because it shows the Avergae Capacity for Azure account storage.

    : Not 
        -> [BlobType] Dimension
        -> [ResponseType] Dimension


# Monitor chart to show how much data uploaded in each data fram. it's being uploaded to Blob storage
    : Metrics Namespace -> Blob
    : Metrics -> Ingress
    : Aggregation (Dimension) -> Sum


    : Not
        -> Metrics Namespace -> account/files/queue cuz our requirements say Blob container
        -> Metrics -> transactions cuz it only shows transactions count not type or size


# Dynamic Data Maskig (DDM) for credit card. onyl 2 first digits and last 4 digits can be visiible
    : use partial function to mask. it recives 3 arguments
        -> Query 
        ALTER TABLE [PAYMENT_CREDIT_CARD]
        ALTER COLUMN [CREDIT_CARD]
        ADD MASKED WITH (FUNCTION = 'partial(2, "xx-xxxx-xxxx-", 4')

    : arg1 -> prefix of how many chars should be showen from the beginging of the calue
    : arg2 -> pattern used for masking 
    : arg3 -> suffix of how many chars are shown from the end.  


# Query encryoed columns (Always Encrypted) 
    : ALTER ANY COLUMN MASTER KEY 
        -> (Required to manage [create and delete] master key, which encrypes/decrypts column encryption keys )

    : ALTER ANY COLUMN ENCRYPTION KEY 
        -> (Required to mange [create and delete] a column encryption key.)

    : VIEW ANY COLUMN MASTER KEY DEFINITION 
        -> (Required to access and read the metadata of the column master keys to manage keys or query encrypted columns.)

    : VIEW ANY COLUMN ENCRYPTION KEY DEFINITION 
        -> (Required to access and read the metadata of the column encryption key to manage keys or query encrypted columns.)



# CosmosDB TTL 
    : Scenario
        single container, that store qulity checks (sucessful checks and defect detections).
        We found out no benefits of keeping sucessful cheeks, so we want to automatically elete those records after an hour.
        and defect detections records must be kept indefinitly.

    : we need to set up in 2 places 
        -> Set TTL in the container to (3600) .. seconds
        -> Programmatily set TTL of [defect detections records] to (-1)

    : This setting will keep all items in the container for an hour, expect defect detections records
    
    : Not
        -> Set TTL in the container to (-1) because this means never expire by default
        -> Programmatily set TTL of [defect detections records] to (null) because that means inhert TTL settings from uuper container.

# Azure SQL Database Auditing
    : Configure Log Analytics as audit log distention
    : Enable Audit log at the server Level. Because this will enable auditing on all dbs in the server

    : Not
        -> Not DB auditing Level
        -> Not Blob storage as a distention becuase you will then need to consume these logs with other services like Databricks and HDInsight


# Azure Stream Anyltics Job Optimization
: Scenario 
    SA job uses 18 SU, ASA mertics and SU percentage utlization usage is 90% from last month
    you need to optimize ASA job 

: we should 
    -> Partion Data for query paralleization
        : partioning data increases job throughput and allows for an optimized use of avialble SU

    -> increase SU count for the job
        : SU represents compute the compute resources allocated for the job
        : Too high SU % utlization indicates a high memeory consumption and higher latency to process events
        : it's recommened to keep SU % below 80%

    -> Query paralleization + high SU count = optimized job Performance



# SQL DWH distribution
    : Replicated Tables for Dims table
        -> for small Dimension tables that change infrequnetly.
        -> By replicating tables across Compute Nodes we elimniate the need for data transfer during JOIN with Fact table
    
    : Hash distribution
        -> use Hash distribution for Fact table.
        -> Hash distribution copies rows across Compute Nodes by using hashing function against column.
        -> Rows with columns that have the same hash value are copied to the same compute Node.

    : Note 
        -> Azure SQL Data Warehouse does not support cross-database queries

# ASA Window functions
    : Scenario
        capture events that repeat and do not overlap. you also want to capture time periods when there is no events !    
    : -> use Tumbling Window
    : Tumbling Window -> it segments event data into distinct time segements and then perform a function against each segement. it allows events repeat but not overlapping

    : Not
        -> Hopping Window, similar to umbling() but it allows overlapping
        -> Sliding Window, it produces output only when event occurs
        -> Session Window, it groups events that arrive at similair times. it filters out periods when there is no data.

    : Functions comparion
        -> Tumbling :
            -> None overlapping
            -> events belong to only one window
            -> fixed-szie time, and continuse

        -> Hopping :
            -> overlapping
            -> fixed-szie time, and continuse
            -> Hopsize : how long a window will overlap with previous Window --> this could result in events belong to one or more than one window

        -> Sliding :
            -> overlapping
            -> fixed-szie time, and continuse
            -> it's similair to setting hopping Window function with a hop size equal to zero, the only Exception here is Sliding Function only produce an output when an event occurs

        -> Session :
            -> Group events that arraive at simnilar time.
            -> events can belong to more than one window
            -> have a variable length


# Run Analytics on data to automatically generate bar charts, and it has to be Scheduled
    : Scheduled Databricks Job
    : Not
        -> Scheduled Runbook in azure automation, becuase it's not generate visual !
        -> Scheduled Azure function, becuase it's not generate visual!.
        -> Scheduled WebJob, becuase it's not generate visual! ** WebJob runs in context of aZURE Service App



# Query Store 
    : Query store can help with things like 
        -> "Creating x number of indexes on able"
        -> Increase VM size. For example, if our SQL on VM

    : Query Store allows to compare Performance before and after an anticopated change.
    : Query Store can only monitor one database.
        -> ALTER DATABASE XXXX SET QUERY_STORE = ON


# Azure Table [row key] and [partioning key]
    : partioning key allows to create logical partions that will enhance the Performance
    : row key, uniquely identifies the row


# HDInsight perofrmance optimization with mini cost.
    : Scenario
        logs files for each server stored in Data Lake Gen2, inside a single folder. and data is analyzed in HDInsight
    : Combine Daily log files into one file.
        -> HDInsight and other Analytics enegin have per-file overhead during processing.
        -> Using small fils will quickly utlize the avialble throughput for Azure Data Lake Gen2 to read data
        -> So, Organizing your data in larger files with szie from 265MBto 100GB result in better performance.

    : Separate the log files into a daily generated folder.
        -> partioning files in time series folders help HDInsightto load only subset of data thus imporve performance.
        -> we can use hierarchic structure like \dataset\yyyy\mm\dd\datafile_yyyy_mm_dd.tsv

    : Not
        -> increase number of worker nodes, it enhance HDinsight throughput, but it increases the cost and don't fix high utlization of ADL throughput
        -> Cool Tier for ADL Gen2, because this tier is optimzied for data that infrequnetly accessed. If we do so in this Scenario, we'll add more data access and transfer cost as we access the log data daily frequnetly


# Elastic Pool with (DTU) provisioning model.
    : Scenario
        Monitor database based on a mertic that best anticopate out-of-Capacity and perofrmance issues.

    : DTU percentage mertic
        -> represents the percentage of DTU that is consumed by database, independent of the number of provisioned DTUs.
        -> if you increase DUT of database, you don't need to edit alert for new Capacity

    : Not
        -> used DTU mertic
            -> it could be used to anticopate perofrmance issues when DTU consumption is close to limit. 
            -> you need to edit this alert if you increase DTU

        -> CPU percentage mertic, Data IO percentage mertic
            -> For DTU based models, CPU, Data IO and storage are bundled together in simplified performance mertic



# Sending Databricks application mertics to Azure Monitor
    : Dropwizard, mertics library
    : Not
        -> Log4j : sending application logs !!! not mertics


# Azure CosmosDB partioning key
    : Scenario
        App spread across 5 regions, and oringe region is an item in CosmoseDB document, and we have sensorID (unique).
        Queries originated from app usuallt filter the result by region and sensorID 
    
    : Region with pre-calculated stuffix based on sensorID
        -> this partioin key will distribute all the documents evenly across logical partions.
        -> Inclduing pre-calculated suffix based on known value (sensorID), will greatly imrpove both write and read throughput across partitions.

    : Not
        -> Region, this field only contains 5 values ! resulting in a small number of logical partioins and a low read and write throughput
        -> sensorID, it's unique which will result in millions of logical partioins, impacting read and write throughput
        -> timestampe with random suffix,
            -> this partion key will distribute all documents across logical partions . 
            -> greatly increase write throughput
            -> HOWEVER, reading specific item will become harder because we don't know which suffix was used, imapcting read throughput



# Azure Data Lake Gen2, ACL in POSIX-compatible format.
    : Scenario
        Alan and Kerry are memebers of Marketing Azure AD secuirty group, set as a primary one for Azure AD accounts.
        Kerry and David are memebers of Finance secuirty group, set as a primary for Davids' account

        You set up a new directry in ADL Gen2, and owner group as Finance.
        Kerry creates a new text file in the directry.
        Your audit report indicates that access control list (ACL), for that file set to 640 in POSIX format.

        You need to determine what access Permission Alan, kerry and David have to the new file.

    : Access will be as :
        -> Alan : No Access
        -> Kerry : Read + write
        -> David : Read only

    : File Permission consists of 3 digits :
        -> Owner : the user who created the item automatically become an Owner
        -> Owner Group : in ADL Gen2, owner group is inherited from the parent folder.
        -> Everyone : 
    
    : ADL Gen2, supports ACL in POSIX format-compatible format. assging numbers to different Permission Combinations. For example:
        -> Read Only : 4
        -> Write Only : 2
        -> Exceute Only : 1
        -> No Access : 0
        -> Read + Write : 4+3 = 6
        -> Read + Execute : 4+1 = 5
        
    : In our Scenario, 640 Permission set on the file can be translated :
        -> Owner (Kerry) - [First digit] : 6, which means READ + WRITE (6)
        -> Owner Group (Finance, which includes David) - [Second digit] : 4, which means READ only () 
        -> Everyone - (Alan) [third digit]: 0, which means No access
    : hierarchical Namespace

# Configure PolyBase to load data from ADL Gen2 to Azure DW, and shoudl NOT use AZure AD service principle.
    1. create a database-scoped credential with the azure storage access key
    2. Create an external data source with HADOOP type.
    3. Create an external file format
    4. create an external table.
    5. load the data inot destination table using (CREATE TABLE *** WITH () AS SELECT * FROM external;)

    : ADL Gen2 use supports the use storage access keys for PolyBase access
    
    : Not 
        ->  "create aa scoped Credential with client id and OAuth 2.0 token endpoint"
            -> you can use this option to connect to ADL Gen2, but it requires azure AD Service principle
    
        -> "BLOB_STORAGE" type for external data source
            -> this type is using when executing bulk operations with on-premises SQL Server or Azure SQL Database.



# Loading data using Polybase vs. Databricks
    : In Databricks, we need temp directory after proccessing data before moving it to SQL DWH
    : In Polybase : No Need

# Import Data into SQL DWH from Azure Storge Blob container
    : Extenal Data Source with HADOOP type.
    : We must use TYPE=HADOOP, when our data source is ADL or Storage Blob container
        ->      TYPE = HADOOP
                LOCATION ='wasbs://xxx@sampl.blob.core.windows.net' --> Blob Storage Endpoint, ADL Gen2 has different Endpoint structure.

        ->      TYPE = HADOOP
                LOCATION = 'abfs[s]://file_system@account_name.dfs.core.windows.net/<path>/<path>/<file_name>' --> ADL Endpoint structure
    : Not 
        -> "BLOB_STORAGE" doesn't represent Azure Blob Storag.
            -> it simply designates the data source as one that will be used with BULK IMPORT or OPENROWSET
            -> it can't be used with external data sources such as Blob storage 
            -> Extenal data source : is one that isn't located in Azure SQL Database.


# Azure SQL Database, Monitor performance by capturing a history of query plan changes over time.
    : ALTER DATABASE XXX SET QUERY_STORE = ON (OPERATION_MODE = READ_WRITE) ... it's disabled by default
    : Not
        -> SQL Server profiler
            -> traces SQL-related events, such as query executions and logins
            

# Choose the right integration and automation services in Azure
    : Microsoft Flow
    : Azure Logic Apps
    : Azure Functions
    : Azure App Service WebJobs


# Access Stroge Blob from Databricks using KeyVault
    : Secret Scopes
        -> Azure Key Vault-backed scopes
        -> Databricks-backed
    # Databricks notebook source
    dbutils.fs.mount(
    source = "wasbs://mycontainer@brciksstore.blob.core.windows.net",
    mount_point  = "/mnt/blobstoremnt",
    extra_configs  = {"fs.azure.account.key.brciksstore.blob.core.windows.net":dbutils.secrets.get(scope = "SecretScopeBlob", key = "dbkStorageKey")})



# Implement Azure Databricks with a Cosmos DB endpoint



# Azure Monitor
    : metrics
        -> are numerical values that describe some aspect of a system at a particular point in tim
        -> tell you how the resource is performing and the resources that it's consuming.

        -> Numerical values only ***
        -> Collected at regular intervals ***
        -> View in Azure portal => Metrics Explorer

    : Logs
        -> contain different kinds of data organized into records with different sets of properties for each type. 
        -> Telemetry such as events and traces are stored as logs in addition to performance data so that it can all be combined for analysis

        -> Text or numeric data ***
        -> May be collected sporadically as events trigger a record to be created.
        -> View in Azure portal => Log Analytics

        :  Log Analytics
            -> to queries the logs

    : Collect monitoring data
        : Different sources of data for Azure Monitor will write to either a 
            -> Log Analytics workspace (Logs) 
            or 
            -> the Azure Monitor metrics database (Metrics) 
            or 
            -> both

    : diagnostics
        -> This will collect telemetry for the internal operation of the resource.

    : Monitoring solutions

        : Application Insights 
            -> monitors the availability, performance, and usage of your web applications whether they're hosted in the cloud or on-premises.

        : Azure Monitor for containers
        : Azure Monitor for VMs
    
    : Alerts in Azure Monitor
        -> Alert rules in Azure Monitor use action groups

# Storage metrics in Azure Monitor
    : metrics

        : Capacity metrics
            : Account Level
                -> UsedCapacity
            : Service Level
                : Blob storage
                    -> BlobCapacity
                    -> BlobCount
                    -> ContainerCount
                    -> IndexCapacity (ADL Gen2)
                : Table storage
                    -> TableCapacity
                    ->
                    ->
                : Queue storage
                    -> 
                    -> 
                    -> 
                : File storage

        : Transaction metrics [available at both account and service level]
            -> Transactions
            -> Ingress
            -> Egress
            -> SuccessServerLatency
            -> SuccessE2ELatency
            -> Availability
    
    : Metrics dimensions
        -> BlobType
        -> BlobTier
        -> GeoType
        -> ResponseType
        -> ApiName
        -> Authentication



# Understanding block blobs, append blobs, and page blobs
    : Type 
        -> Block
        -> appeand
        -> page
    : Blob lease 
        -> lease id or its blocks



# Streaming Unit (SU)
    : Streaming Units (SUs) represents the computing resources that are allocated to execute a Stream Analytics job. 
    : The higher the number of SUs, the more CPU and memory resources are allocated for your job

    : To achieve low latency stream processing, Azure Stream Analytics jobs perform all processing in memory

    : SU % utilization metric
        -> which ranges from 0% to 100%
        -> describes the memory consumption of your workload.

        -> For a streaming job with minimal footprint, this metric is usually between 10% to 20%. 
        -> If SU% utilization is low and input events get backlogged, your workload likely requires more compute resources, which requires you to increase the number of SUs. 
        -> It’s best to keep the SU metric below 80% to account for occasional spikes.
        -> Microsoft recommends setting an alert on 80% SU Utilization metric to prevent resource exhaustion

        -> 80% utlization probaly means job will fail
        -> use it with MAx Aggregation to get used capacity
        -> Increasing number of streaming units for a job might not reduce SU% Utilization if your query is not fully parallel.

        ->  PARTITION BY


# SQL Server Encryption 
    : Encryption Mechanisms
        -> Transact-SQL functions
        -> Asymmetric keys
        -> Symmetric keys
        -> Certificates
        -> Transparent Data Encryption

    : Transparent Data Encryption (TDE)
        === use ===> symmetric key called the Database Encryption Key (DEK)
                    === protected by ===> Data encryption protector (TDE protector)
                                        === either  === > service-managed certificate in the master database of the server 
                                                    === > or  
                                                    ===> asymmetric key protected by an EKM module or Azure Key Vault

    : Always Encrypted
        ===> Column Encryption keys (CEK) : to encrypt data in the database, and they're stored in the database in the encrypted form
        ===> Column Master keys (CMK) :  to encrypt encryption keys, are stored in an external key store

        : supports two types of encryption: 
            -> randomized encryption
                : always generates the same encrypted value for any given plain text value. 
                : Using deterministic encryption allows point lookups, equality joins, grouping and indexing on encrypted columns.
                : it may also allow unauthorized users to guess information about encrypted values by examining patterns in the encrypted column

            -> deterministic encryption.
                : uses a method that encrypts data in a less predictable manner
                : Randomized encryption is more secure, but prevents searching, grouping, indexing, and joining on encrypted columns.





    : TDE is the recommended choice for encryption at rest, and we recommend TLS for protecting data in-transit
    : Recommendation :
        ==> TDE to encrypt the entire database at rest
            ==> TLS to protect all traffic to the database.
                ==> Always Encrypted to protect highly sensitive data from high-privilege users and malware.



# Row-Level Security
    : query 
        CREATE SECURITY POLICY SalesFilter  
        ADD FILTER PREDICATE Security.fn_securitypredicate(SalesRep)
        ON dbo.Sales  
        WITH (STATE = ON);  
    
    : parts :
        -> SECURITY POLICY
        -> FILTER PREDICATE ==> [fn_securitypredicate(SalesRep)]   function name and its input
        -> ON ==> table name


# Re-encrypt Transparent Data Encryption (TDE), using KeyVault, powershell
    : Assign AAD to Azure SQL
    : Grant KeyVault Permission to Azure SQL 
    : Add KeyVault to Azure SQL, and set it as TED protector
    : Turn on TED 

    :-> if you do this via the portal, you DO NOT Need Step 1 & 2 :
        you get an AAD identity assgined and relevant Permissions set to Azure SQL via Azure KeyVault access policy automatically.



# Encrypt Transparent Data Encryption (TDE), using KeyVault, in geo-replicate in 2 regions :
    : start with secondary region.
    : On secondary, assgin KeyVault that in the same region
    : on secondary, assgin TDE protector

    : do the same in primary region 



# Easily identify queries within DMV in SQL :
    : use LABLE option

#  Azure Databricks Monitoring
    : default format for mertics data used by Azure Databricks => Ganglia
    : supports sending logs to Azure monitor => via 3rd party library
    : Mertics transfoned to Azure will be stored in : Azure Log Analytics workspace


# In-Memeory storage (In-Memeory OLTP) utlization
    : SELECT xtp_storage_percentage from sys.dm_db_resource_stats 
    Or
    : DB > Monitor > mertics blade > in-memory OLTP storage percentage mertic 


# Azure Event Hubs
    : allows you to capture, retain ,and replay telemetry data.
    : it accepts data stream over HTTP and AMQP

# Azure IoT Hub
    : it accepts data stream over HTTP, AMQP, MQTT

# Azure Event Gird
    : publish-subscribe platform for events
    : Events publisher, sends events to Event Grid. And Subscriber subsbcribe to the events they want to handle.
# Azure Relay
    : Allows client apps to connect to services hosted on a private networkr over internet
    : allows client apps to access on-premise services through Azure
    : it (Azure Functions as well) doesn't accepts messages over AMQP


# Dynamic Data Maskig (DDM) vs Row Level Security (RLS)
    : DDM -> allows limiting the exposure of specific fields
    : RLS -> Hide the whole row not just fields/columns

# DWH re-partioin
    : Disable the columnstore index 
    : user ALTER TABLE statement with SPLIT clause
    : Rebuild the columnstore index

# Azure IT Service Management Connector (ITSMC)

# Azure Data Factory pipeline Monitoring 
    : To store ADF pipeline run history more than the default (45) we can send logs to Azure Monitor by configuring Diagnostics logs to send data to Blob Storag

# give an existing user Admin rights to DB
    : ALTER ROLE db_owner ADD MEMEBER matar
    : Not
        - ALTER USER 

# TDE with customer-manged encryption key
    -> Assuming : Azure AD identity is already assgined in Azure SQL DB Server
    1. Create Azure KeyVault and generate new key
    2. Grant KeyVault Permissions (getm weapKey, UnwrapKey) to that Azure AD identity assgined to Azure SQL Server
    3. Add KeyVault Key to Azure SQL Server
    4. Set TDE protector to use the KeyVault key
    5. Enable encryption in DB

    : Not
        -> Create MAster key in master DB,  create server certificate usin master key, create encryption ket from that certificate
            => this setup for configuring TDE for  On-premise SQL 
    : By Default, TDE is enabled in new provisoned Azure SQL using services-manged encryption key


# Azure Data Factory
    : Scenario
        connect to on-premise db and move data periodically to Azure SQL DB, the pipeline is allowed to run at specific, fixed-size time interval
    : Component -> Linked Service
    : Execution -> Tumbling Window in Pipeline execution trigger 

# Azure CosmosDB CLI 
    : az [cosmosdb] => to create account
    : az [cosmosdb creat] => to creat DB
    : --kind GlobalDocumentDB
    : --default-consistency-level Strong
    : --enable-multiple-write-locations => provision CosmosDB in multi-regions with multiple write regions
    : --enable-automatic-failover => if you want to enable automatic failover in case of Disatir

# Watermark approach in Azure Data Factory
    1. lookup the value from Watermark table
    2. lookup the value from customer table
    3. copy delta using Watermark
    4. execute SP to update Watermark table

# Log API calls to ADL Gen2
    : Scenario
        logs should be stored in BLOB Storag
        only include operations about API calls 
    : From Diagnostics Setting blad config 
        -> LOG section enable ==> Requests ==> capture every API request made to the account
        -> LOG section enable ==> Audit ==> to capture breakdown of the operation made to APIs
        
# SQL Secuirty policy Predicate 
    CREATE SECURITY POLICY SalesFilter_ext
    ADD FILTER PREDICATE Security.fn_securitypredicate(SalesRep)
    ON dbo.Sales_ext  
    WITH (STATE = ON);

# Configure number of DTU for multiple DBs in Elastic pool
    : determine number of total DTUs that are used by all DBs combined 

# Access ADL Gen2 from Azure Databricks
    1. Create Databricks secret scope in the workspace
    2. Add ADL Gen2 account key and a service principle seconds to that scope
    3. Configure the storage account key wwith spark.conf.set().
        => to ensure that storage account reads the key from secret scope with dbutils.secrets.get() method
    4. mount a filesystem using a service principle
        => to securly mount the filesystem using service principle store in the secret scope.


    : Not 
        -> spark.sparkContext.hadoopConfigration()
            => you can use this to read directly from ADL Gen2, HOWEVER; storage account key will be exposed to all users who access the cluster

        -> Read from storage account using RDD API
            => this will requires you to use spark.sparkContext.hadoopConfigration()

        -> You can't use SAS with ADL Gen2

# Azure messaging services
    : Event Grid
    : Event Hubs
    : Service Bus


# Scenario : build e-commerce .. CosmosDB Table API vs. Azure Table Storag, the solution must allow .NET requests
    : yes both meet the requirements
        -> Azure Table Storag => uses NoSQL, which allow you to store ( keys and attributes ) in schemaless store
        -> this is similar to CosmosDB with Table API
        -> Each Entity (row), can store a varyinb number of attributes (Fields)

    : SQL API is correct too.
        -> it allows you to work with JSON, tree-shape data instead of rows & columns


# Azure Data Factory - migrate data from an on-premises Hadoop cluster to Azure Storage
    : Scenario
        huge data stored on-premise in Hadoop cluster.
        you want to copy data to Azure Storage
        On-premise datacenter is connnected to Azure VNet with Express Route peering.
        Azure Storag endpoint is accessible from same VNet 
        You aren't allowed to transfer data over public internet

    : Use Azure Data Factory 
        => in Native Integratio Runtime (IR) mode
            => with ADF self-hosted IR machine installed on the Azure Vnet
        -> This approach supports data transfer via Express Route and use [ ADF IR as as engine to copy the data ]


    : above solution will be different if we were allowed to use public internet
        -> we can use ADF in Distributed copy (DistCp) mode 
        -> in this case, n install ADF IR on our local machine, not into VM in Azure VNet 
        -> DistCp : doesn't support Express Route private peering with Azure Storage Vnet endpoint

    : Source 
        -> https://docs.microsoft.com/en-us/azure/data-factory/data-migration-guidance-hdfs-azure-storage

    : Details 
        Data Factory DistCp mode (recommended): 
            -> you can use DistCp (distributed copy) to copy files as-is to Azure Blob storage (including staged copy) or Azure Data Lake Store Gen2. 
            -> Use Data Factory integrated with DistCp to take advantage of an existing powerful cluster to achieve the best copy throughput. 
            -> Depending on your Data Factory configuration, copy activity automatically constructs a DistCp command, submits the data to your Hadoop cluster, and then monitors the copy status.
        
            => We recommend Data Factory DistCp mode for migrating data from an on-premises Hadoop cluster to Azure.
            => DistCp : doesn't support Express Route private peering with Azure Storage Vnet endpoint
    
        Data Factory native integration runtime mode: 
            -> in an Azure VNet environment, the DistCp tool doesn't support Azure ExpressRoute private peering with an Azure Storage virtual network endpoint. 
            -> In addition, in some cases, you don't want to use your existing Hadoop cluster as an engine for migrating data so you don't put heavy loads on your cluster, 
            -> Instead, you can use the native capability of the Data Factory integration runtime as the engine that copies data from on-premises HDFS to Azure.


    : Solution architecture (https://docs.microsoft.com/en-us/azure/data-factory/data-migration-guidance-hdfs-azure-storage)
        -> architecture (A) : migrating data over the public internet (HTTPS or ExpressRoute MS Peering)

            => using Data Factory DistCp mode in a public network environment
            => you must install the Data Factory self-hosted integration runtime on a Windows machine behind a corporate firewall to submit the DistCp command to your Hadoop cluster and to monitor the copy status
            => Because the machine isn't the engine that will move data (for control purpose only), the capacity of the machine doesn't affect the throughput of data movement.

        -> architecture (B) :  migrating data over a private link

            => data is migrated over a private peering link via Azure ExpressRoute. Data never traverses over the public internet.
            => The DistCp tool doesn't support ExpressRoute private peering with an Azure Storage virtual network endpoint. 
            => We recommend that you use Data Factory's native capability via the integration runtime to migrate the data.
            => you must install the Data Factory self-hosted integration runtime on a Windows VM in your Azure virtual network ***



    : The Integration Runtime (IR) 
        -> is the compute infrastructure used by Azure Data Factory to provide data integration capabilities across different network environments
        -> It is hosted in Azure environment and supports connecting to resources in public network environment with public accessible endpoints.
        -> it's called ( Default Azure IR )

    : Self-hosted integration runtime 
        -> can run copy activities between a cloud data store and a data store in a private network. 
        -> It also can dispatch transform activities against compute resources in an on-premises network or an Azure virtual network. 
        -> The installation of a self-hosted integration runtime needs an on-premises machine or a virtual machine inside a private network.


# Databricks Access token
    : Allows external application to access Databricks

# on-premise SSIS to Azure Data Factory
    : Scenario
        ERP on SQL Server on-premise. Another, SQL Server configured with SSIS packages to extract data from ERP to on-premise SQL DWH.
        you need to prepare SSIS process to be integrated with Azure Data Factory for future migration to the cloud.

        You need to configure self-hosted integration runtime (IR) as proxy for Azure-SSIS IR in ADF
        You already created Azure Blob storage for the integration

    : actions to perform 
        1. Create a linked serivce in ADF for Storag account 
        2. Create Azure-SSIS IR in ADF
        3. Install self-hosted IR the on-premise SSIS
        4. Set up the self-hosted IR as a proxy for your Azure-SSIS IR 
        5. Regierser the self-host IR with Authentication key 

    : With this feature 
        -> you can access data on-premises without having to join your Azure-SSIS IR to a virtual network. 
        -> The feature is useful when your corporate network has a configuration too complex or a policy too restrictive for you to inject your Azure-SSIS IR into it.


# Optimal Performance for ASA Job :
    : you should start with 6 SUs for queries don't use PARTITION BY. it's a best practice.
    : you shuld keep SU mertics below 80 %. it's a best practice.
    : you should allocate more SUs than you need. it's a best practice.


# powershell, Azure Monitor, Log Analytics !

# SQL Server Profiler Trace 
    : to catch Stored Procedure that being executed

# Synch Azure SQL DB to newly created Azure SQL DB in another region. Note that DB is being used by distributed app with Read/Write Permission
    : Azure SQL Data Sync 
        -> allows sync data across multiple Azure SQL, and on-premise as well, bi-directionally
    
    : Not 
        -> Azure SQL geo-replication
            : it's a disaster recovey solution.
            : the sysnc direction is only from Master to the replica DB 
            : you only have read access to the replica 


# Azure Databricks with Azure Event Hubs 
    : .setEventHubName("MyEHName)
    : var eventhubs =spark.readStream.format("eventhubs").options(evezzzz).load()


# TED and Always Encrypted
    : only specific columns in 10 tables must be encrypted => Alwyas encrypted
    : Entire DB encrypted at rest => TED 
    : Data must be encrypted in trasnit between client and Azure => Alwyas encrypted

# copy data from Azure Databricks to ADL Gen2
    : dbutils.fs.cp("file:///tmp/ss.csv", "abfss://xxx@sample.dfs.core.windows.net/")

# SQL performance issues
    : use Columnstore Index , good for Aggregation queries 
    : Memory-optimized table, store all data & schema in memeory 

# ASA Job stop unexpectedly 
    : All adminstrative operations single logic Log 


# Azure Blob storage lifecycle

    {
            "rules": [
            {
            "name": "rule1",
            "enabled": true,
            "type": "Lifecycle",
            "definition": {...}
            },
            {
            "name": "rule2",
            "type": "Lifecycle",
            "definition": {...}
            }
        ]
    }

    
    : A policy is a collection of rules
        -> rules[] : An array of rule objects
        baseBlob
    : 


    : Sample Rule 

        {
        "rules": [
            {
            "name": "ruleFoo",
            "enabled": true,
            "type": "Lifecycle",
            "definition": {
                "filters": {
                "blobTypes": [ "blockBlob" ],
                "prefixMatch": [ "container1/foo" ]
                },
                "actions": {
                "baseBlob": {
                    "tierToCool": { "daysAfterModificationGreaterThan": 30 },
                    "tierToArchive": { "daysAfterModificationGreaterThan": 90 },
                    "delete": { "daysAfterModificationGreaterThan": 2555 }
                },
                "snapshot": {
                    "delete": { "daysAfterCreationGreaterThan": 90 }
                }
                }
            }
            }
        ]
        }

    : baseBlob vs Snapshot 
        "actions": {
            "baseBlob": {

        "actions": {
          "snapshot": {


    : Rule filters
        -> blobTypes
        -> prefixMatch
    
    : Rule actions
        -> tierToCool
        -> tierToArchive
        -> delete

# Linked Service vs. Runtime (ex: self-hosted)





************************************************************************************ practice Test 02 ************************************************************************************

You are developing a solution to visualize multiple terabytes of geospatial data.
The solution has the following requirements:
-> Data must be encrypted.
-> Data must be accessible by multiple resources on Microsoft Azure.

    1. Create ADL with Azure KeyVault managed encryption keys
    2. select and configure an encryption key storage container 
    3. add an access policy for ADL account to the key storage container 
    4. enable encryption on ADL using the portal 


---

You are developing a data engineering solution for a company. The solution will store a large set of key-value pair data by using Microsoft Azure Cosmos DB.
The solution has the following requirements:
-> Data must be partitioned into multiple containers.
-> Data containers must be configured separately.
-> Data must be accessible from applications hosted around the world.
-> The solution must minimize latency.

    1. Provision an Azure Cosmos DB account with the Azure Table API. Enable multi-region writes.

---

affect your costs when sizing the Azure SQL Database elastic pools
B. number of databases
C. eDTUs consumption
---

Copy data from on-premises SQL Server instance to Azure Blob storage.

    Step 1: Deploy an Azure Data Factory
        You need to create a data factory and start the Data Factory UI to create a pipeline in the data factory.
    Step 2: From the on-premises network, install and configure a self-hosted runtime.
        To use copy data from a SQL Server database that isn't publicly accessible, you need to set up a self-hosted integration runtime.
    Step 3: Configure a linked service to connect to the SQL Server instance.
--- 

A company runs Microsoft SQL Server in an on-premises virtual machine (VM).
You must migrate the database to Azure SQL Database. You synchronize users from Active Directory to Azure Active Directory (Azure AD).
You need to configure Azure SQL Database to use an Azure AD user as administrator.
What should you configure?

    A. For each Azure SQL Database, set the Access Control to administrator.
    B. For each Azure SQL Database server, set the Active Directory to administrator.
    C. For each Azure SQL Database, set the Active Directory administrator role. ******** !!!
    D. For each Azure SQL Database server, set the Access Control to administrator.

    There are two administrative accounts act as administrators:
        1. Server admin
        2. Active Directory admin

    One Azure Active Directory account, either an individual or security group account, can also be configured as an administrator. 
    It is optional to configure an Azure AD administrator, but an Azure AD administrator must be configured if you want to use Azure AD accounts to connect to SQL Database.
---

You plan to create a dimension table in Azure Data Warehouse that will be less than 1 GB.
You need to create the table to meet the following requirements:
-> Provide the fastest query time.
-> Minimize data movement.
Which type of table should you use?
    A. hash distributed
    B. heap
    C. replicated ** -> the table is small, and our requirements are minimal movment & faster queries

    Usually common dimension tables or tables that doesnâ€™t distribute evenly are good candidates for round-robin distributed table.
    Note: Dimension tables or other lookup tables in a schema can usually be stored as round-robin tables. 
    Usually these tables connect to more than one fact tables and optimizing for one join may not be the best idea. 
    Also usually dimension tables are smaller which can leave some distributions empty when hash distributed.
    Round-robin by definition guarantees a uniform data distribution.
--- 

You have an Azure SQL data warehouse.
Using PolyBase, you create table named [Ext].[Items] to query Parquet files stored in Azure Data Lake Storage Gen2 without importing the data to the data warehouse.
The external table has three columns.
You discover that the Parquet files have a fourth column named ItemID.
Which command should you run to add the ItemID column to the external table?


    DROP TABLE
    CREATE EXTERNAL TABLE XXX

    NOT :
    ALTER TABLE

    **** Only these Data Definition Language (DDL) statements are allowed on external tables:

        CREATE TABLE and DROP TABLE
        CREATE STATISTICS and DROP STATISTICS
        CREATE VIEW and DROP VIEW
---

DWH
    DELETE PARTIONS CONTAIN OLD DATA 
--

You plan to implement an Azure Cosmos DB database that will write 100,000 JSON every 24 hours. The database will be replicated to three regions. Only one region will be writable.
You need to select a consistency level for the database to meet the following requirements:
    -> Guarantee monotonic reads and writes within a session.
    -> Provide the fastest throughput.
    -> Provide the lowest latency.
    Which consistency level should you select?

        Session: 
            Within a single client session reads are guaranteed to honor the consistent-prefix (assuming a single writer session), 
                monotonic reads, 
                monotonic writes, 
                read-your-writes, 
                and write-follows-reads guarantees. 
            Clients outside of the session performing writes will see eventual consistency.
--- 

You have an Azure Storage account that contains 100 GB of files. The files contain text and numerical values. 75% of the rows contain description data that has an average length of 1.1 MB.
You plan to copy the data from the storage account to an Azure SQL data warehouse.
You need to prepare the files to ensure that the data copies quickly.
Solution: You modify the files to ensure that each row is less than 1 MB.
    Does this meet the goal? 
    NO
    Instead modify the files to ensure that each row is less than 1 MB.

---

# CosmosDB Consistency levels and data durability

You plan to deploy an Azure Cosmos DB database that supports multi-master replication.
You need to select a consistency level for the database to meet the following requirements:
-> Provide a recovery point objective (RPO) of less than 15 minutes.
-> Provide a recovery time objective (RTO) of zero minutes.
What are three possible consistency levels that you can select? Each correct answer presents a complete solution.
NOTE: Each correct selection is worth one point.
    A. Strong
    B. Bounded Staleness
    C. Eventual
    D. Session
    E. Consistent Prefix

    Answer : C D E
---

# Storage account access tiers
You need to ensure that you can recover any blob data from an Azure Storage account named storage xxxxx up to 30 days after the data is deleted.

    ->  The cool access tier has lower storage costs and higher access costs compared to hot storage. This tier is intended for data that will remain in the cool tier for at least 30 days.
---

    Box 1: Yes - the query joins 2 streams of partioned data
        You can now use a new extension of Azure Stream Analytics SQL to specify the number of partitions of a stream when reshuffling the data.

        The outcome is a stream that has the same partition scheme. Please see below for an example:
        WITH    step1 AS (SELECT * FROM [input1] PARTITION BY DeviceID INTO 10), 
                step2 AS (SELECT * FROM [input2] PARTITION BY DeviceID INTO 10)

        SELECT * INTO [output] 
        FROM    step1 PARTITION BY DeviceID 
                UNION 
                step2 PARTITION BY DeviceID
        
        Note: The new extension of Azure Stream Analytics SQL includes a keyword INTO that allows you 
                to specify the number of partitions for a stream when performing reshuffling using a PARTITION BY statement.


    Box 2: Yes -
        When joining two streams of data explicitly repartitioned, 
        these streams must have the same partition key and partition count.

    Box 3: Yes - providing 60 SUs will optimizate perforamnce of the quer
        10 partitions x six SUs = 60 SUs is fine.

        Note: Remember, Streaming Unit (SU) count, which is the unit of scale for Azure Stream Analytics, 
        must be adjusted so the number of physical resources available to the job can fit the partitioned flow. 
        In general, **** six SUs *** is a good number to assign to each partition. In case there are insufficient resources assigned to the job, 
        the system will only apply the repartition if it benefits the job.

---

You need to ensure that users in the West US region can read data from a local copy of an Azure Cosmos DB database
    enable the Geo-redundancy, Multi-region Writes, Avialibty Zone
    e Replicate data globally menu -> add region
---

You need to ensure that User1-10543936@ExamUsers.com can manage any databases hosted on an Azure SQL server named SQL10543936 by signing in using his Azure Active Directory (Azure AD) user account.
    Provision an Azure Active Directory administrator for your managed instance
    Each Azure SQL server (which hosts a SQL Database or SQL Data Warehouse) starts with a single server administrator account that is the administrator of the entire Azure SQL server. A second SQL Server administrator must be created, that is an Azure AD account. This principal is created as a contained database user in the master database.
    In SQL Server page, select Active Directory admin.
    n the Active Directory admin page, select Set admin
---

deployment using DevOps
    create deployment artifacts caonting an extraced AMR template
    paramtrize deployment using ARM template paramerters
    configure azure devops to deploy the deployment artifacts


    You need to provision the polling data storage account :
        Account type: StorageV2 -
        Replication type: RA-GRS ---> why Not GRS ???
            -> GRS replicates your data to another data center in a secondary region, 
                but that data is available to be read only if Microsoft initiates a failover from the primary to secondary region.

            -> Read-access geo-redundant storage (RA-GRS) is based on GRS. RA-GRS replicates your data to another data center in a secondary region, 
                and also provides you with the option to read from the secondary region. 
                With RA-GRS, you can read from the secondary region regardless of whether Microsoft initiates a failover from the primary to secondary region.
--

Overview -

General Overview -
Litware, Inc, is an international car racing and manufacturing company that has 1,000 employees. Most employees are located in Europe. The company supports racing teams that complete in a worldwide racing series.

Physical Locations -
Litware has two main locations: a main office in London, England, and a manufacturing plant in Berlin, Germany.
During each race weekend, 100 engineers set up a remote portable office by using a VPN to connect the datacentre in the London office. The portable office is set up and torn down in approximately 20 different countries each year.

Existing environment -

Race Central -
During race weekends, Litware uses a primary application named Race Central. Each car has several sensors that send real-time telemetry data to the London datacentre. The data is used for real-time tracking of the cars.
Race Central also sends batch updates to an application named Mechanical Workflow by using Microsoft SQL Server Integration Services (SSIS).
The telemetry data is sent to a MongoDB database. A custom application then moves the data to databases in SQL Server 2017. The telemetry data in MongoDB has more than 500 attributes. The application changes the attribute names when the data is moved to SQL Server 2017.
The database structure contains both OLAP and OLTP databases.

Mechanical Workflow -
Mechanical Workflow is used to track changes and improvements made to the cars during their lifetime.
Currently, Mechanical Workflow runs on SQL Server 2017 as an OLAP system.
Mechanical Workflow has a named Table1 that is 1 TB. Large aggregations are performed on a single column of Table 1.

Requirements -

Planned Changes -
Litware is the process of rearchitecting its data estate to be hosted in Azure. The company plans to decommission the London datacentre and move all its applications to an Azure datacentre.

Technical Requirements -
Litware identifies the following technical requirements:
Data collection for Race Central must be moved to Azure Cosmos DB and Azure SQL Database. The data must be written to the Azure datacentre closest to each race and must converge in the least amount of time.
The query performance of Race Central must be stable, and the administrative time it takes to perform optimizations must be minimized.
The datacentre for Mechanical Workflow must be moved to Azure SQL data Warehouse.
Transparent data encryption (IDE) must be enabled on all data stores, whenever possible.
An Azure Data Factory pipeline must be used to move data from Cosmos DB to SQL Database for Race Central. If the data load takes longer than 20 minutes, configuration changes must be made to Data Factory.
The telemetry data must migrate toward a solution that is native to Azure.
The telemetry data must be monitored for performance issues. You must adjust the Cosmos DB Request Units per second (RU/s) to maintain a performance
SLA while minimizing the cost of the Ru/s.

Data Masking Requirements -
During rare weekends, visitors will be able to enter the remote portable offices. Litware is concerned that some proprietary information might be exposed. The company identifies the following data masking requirements for the Race Central data that will be stored in SQL Database:
Only show the last four digits of the values in a column named SuspensionSprings.
Only Show a zero value for the values in a column named ShockOilWeight.

    ** You need to build a solution to collect the telemetry data for Race Control ?
        API -> Table 
            Azure Cosmos DB provides native support for wire protocol-compatible APIs for popular databases. These include MongoDB, Apache Cassandra, Gremlin, and
            Azure Table storage.
            Scenario: The telemetry data must migrate toward a solution that is native to Azure.

        Consistency Level -> Strong
            Use the strongest consistency Strong to minimize convergence time.
            Scenario: The data must be written to the Azure datacentre closest to each race and must converge in the least amount of time.

    ** On which data store you configure TDE to meet the technical requirements?
        SQL Data Warehouse ->
            Scenario: Transparent data encryption (TDE) must be enabled on all data stores, whenever possible.
            The datacentre for Mechanical Workflow must be moved to Azure SQL data Warehouse.
            
            Incorrect Answers:
            A: Cosmos DB does not support TDE.

    ** You are building the data store solution for Mechanical Workflow. How should you configure Table1? 
        Table Type -> Hash distributed
        Index type -> clustered columnstore

    ** Which masking functions should you implement for each column to meet the data masking requirements? To answer, select the appropriate options in the answer area.
        ShockOilWeight -> Default
        SuspensionSpring -> Credit card 

---

A company plans to analyze a continuous flow of data from a social media platform by using Microsoft Azure Stream Analytics. The incoming data is formatted as one record per row.
You need to create the input stream.
How should you complete the REST API segment? To answer, select the appropriate configuration in the answer area.

    Box 1: CSV - *********
        A comma-separated values (CSV) file is a delimited text file that uses a comma to separate values. A CSV file stores tabular data (numbers and text) in plain text.
        Each line of the file is a data record.
        JSON and AVRO are not formatted as one record per row.


    Box 2: "type":"Microsoft.ServiceBus/EventHub",
        Properties include "EventHubName"
---

A company plans to develop solutions to perform batch processing of multiple sets of geospatial data.
You need to implement the solutions.
Which Azure services should you use? To answer, select the appropriate configuration in the answer area.

    Box 1: HDInsight Tools for Visual Studio, use a native client app to run Interactive queries and batch processing
        Azure HDInsight Tools for Visual Studio Code is an extension in the Visual Studio Code Marketplace for developing Hive Interactive Query, Hive Batch Job and
        PySpark Job against Microsoft HDInsight.

    Box 2: Hive View - use web browser to run Interactive queries and batch processing
        You can use Apache Ambari Hive View with Apache Hadoop in HDInsight. The Hive View allows you to author, optimize, and run Hive queries from your web browser.

    Box 3: HDInsight REST API - develop batch processing app using Azure HDInsight
        Azure HDInsight REST APIs are used to create and manage HDInsight resources through Azure Resource Manager.

--- 
You develop data engineering solutions for a company.
You need to deploy a Microsoft Azure Stream Analytics job for an IoT solution. The solution must:
-> Minimize latency.
-> Minimize bandwidth usage between the job and IoT device.
Which four actions should you perform in sequence?

    Step 1: Create an IoT hub and add the Azure Stream Analytics module to the IoT Hub namespace
        An IoT Hub in Azure is required.

    Step 2: Create an Azure Blob Storage container
        To prepare your Stream Analytics job to be deployed on an IoT Edge device, you need to associate the job with a container in a storage account. 
        When you go to deploy your job, the job definition is exported to the storage container.
        Stream Analytics accepts data incoming from several kinds of event sources including Event Hubs, IoT Hub, and Blob storage.
    
    Step 3: Create an Azure Stream Analytics edge job and configure job definition save location
        When you create an Azure Stream Analytics job to run on an IoT Edge device, it needs to be stored in a way that can be called from the device.

    Step 4: Configure routes -
        You are now ready to deploy the Azure Stream Analytics job on your IoT Edge device.
        The routes that you declare define the flow of data through the IoT Edge device.

---

Box 1: Apache Sqoop -
    Apache Sqoop is a tool designed for efficiently transferring bulk data between Apache Hadoop and structured datastores such as relational databases.


Box 2: Apache Kafka -
    Apache Kafka is a distributed streaming platform.
    A streaming platform has three key capabilities:
        1. [Publish and subscribe] to streams of records, similar to a message queue or enterprise messaging system.
        2. [Store streams] of records in a fault-tolerant durable way.
        3. [Process streams] of records as they occur.

    Kafka is generally used for two broad classes of applications:
        - Building real-time streaming data pipelines that reliably get data between systems or applications
        - Building real-time streaming applications that transform or react to the streams of data

Box 3: Ambari Hive View -
    You can run Hive queries by using Apache Ambari Hive View. 
    The Hive View allows you to author, optimize, and run Hive queries from your web browser.

---

SQLPackage utility
---

A role assignment consists of three elements: 
    security principal, 
    role definition, 
    scope.
---

D. Create security groups in Azure Active Directory (Azure AD) and add project members.
A. Assign Azure AD security groups to Azure Data Lake Storage.
E. Configure access control lists (ACL) for the Azure Data Lake Storage account.
-----

Step 1: From the Azure Portal, set the Active Directory admin for SQL1.
    Provision an Azure Active Directory administrator for your Azure SQL Database server.
    You can provision an Azure Active Directory administrator for your Azure SQL server in the Azure portal and by using PowerShell.


Step 2: On DB1, create a contained user for the Analysts group by using Transact-SQL
    Create contained database users in your database mapped to Azure AD identities.
    To create an Azure AD-based contained database user (other than the server administrator that owns the database), connect to the database with an Azure AD identity, as a user with at least the ALTER ANY USER permission. Then use the following Transact-SQL syntax:
        CREATE USER <Azure_AD_principal_name> FROM EXTERNAL PROVIDER;


Step 3: From Microsoft SQL Server Management Studio (SSMS), sign in to SQL1 by using the account set as the Active Directory admin.
    Connect to the user database or data warehouse by using SSMS or SSDT
    To confirm the Azure AD administrator is properly set up, connect to the master database using the Azure AD administrator account. To provision an Azure AD- based contained database user (other than the server administrator that owns the database), connect to the database with an Azure AD identity that has access to the database.


Step 4: On DB1, grant the VIEW and SELECT DEFINTION..

----

Azure SQL -> Advanced Data Secuirty -> Advanced Threat protection settings -> send an email 
---

classify nformation as Confidential in Azure SQL 
    Security and Advance Data Security -> Enable advanced Data Security Protection -> Data Discovery & Classification
---

Azure SQL prevent data leakage.
    Step 1: Enable advanced threat protection
    Step 2: Configure the service to send email alerts to security@contoso.team
    Step 3: Configure the service to create alerts for threat detections of type data exfiltration
--- 

Azure Cosmos DB -> types of keys
    two types of keys to authenticate users and provide access to its data and resources

    1) Master Key
        Master keys provide access to the all the administrative resources for the database account.
        Provide access to accounts, databases, users, and permissions.

    2) Resource Tokens
        Resource tokens provide access to the application resources within a database.
        to let external clien to query the database 
---

Step 1: Create a master key using password
Step 2: Create or obtain a certificate protected by the master key

Step 3: Set the context to the company database

Step 4: Create a database encryption key and protect it by the certificate
Step 5: Set the database to use encryption
---

enbale soft delete on Blob account
---

DataBase Scoped Credential :
    SQL Server uses a database scoped credential to access non-public Azure blob storage 
        or Kerberos-secured Hadoop clusters with PolyBase.

    PolyBase cannot authenticate by using Azure AD authentication.
---

1. Access the Always Encrypted Wizard in SQL Server Management Studio
2. Select the column to be encrypted
3. Set the encryption type to Randomized
4. Configure the master key to use the Windows Certificate Store *** -> wrong, it should be azure KeyVault in this Scenario
5. Validate configuration results and deploy the solution
---

partial masking = custom masking
---

Tier 7 and Tier 8 partner access must be restricted to the database only ... SQL Server and DB firewall rules

    Step 1: Set the Allow Azure Services to Access Server setting to 
        By default, access through the SQL Database firewall is enabled for all Azure services, 
    
    Step 2: In the Azure portal, create a server firewall rule
        Set up SQL Database server firewall rules .. apply to all DBs in the server !
        we still need to scope it down to DB level 

    Step 3: Connect to the database and use Transact-SQL to create a database firewall rule
        Database-level firewall rules can only be configured using Transact-SQL (T-SQL) statements, and only after you've configured a server-level firewall rule.
        ex: -> EXECUTE sp_set_database_firewall_rule N'Example DB Rule','0.0.0.4','0.0.0.4';
---

DWU limit vs DWU used 
---

identify when a user attempts to infer data from the masked columns -> audting 
---

automatic tuning on Azure SQL Database logical server
---

You need to recommend a tool that will monitor clusters and provide information to suggest how to scale.

    Ambari Web UI does not provide information to suggest how to scale.
    Instead monitor clusters by using Azure Log Analytics and HDInsight cluster management solutions.


************************************************************************************ End practice Test 02 ************************************************************************************

# on-premises data gateway

# database access to SQL Database and Azure Synapse Analytics
    -> Two administrative accounts that act as administrators
        1. Server admin
        2. Active Directory admin

    ->  CREATE USER
        => ** User with Azure Active Directory
            CREATE USER [mike@contoso.com] FROM EXTERNAL PROVIDER; 
        
        => ** SQL Database contained database user
            CREATE USER Ann WITH PASSWORD = '<strong_password>';
        
        => ** SQL Server user based on a SQL Server authentication login
            CREATE USER Mary FROM LOGIN Mary;


# create Azure AD SQL user 
    1. create Azure AD Admin, the assign it as SQL Server Admin
        storage@maalhawitihotmail.onmicrosoft.com

    2. Using Azure AD admin, add Azure AD user to spcific AD DB 
        -- master
        CREATE USER [matar-test@maalhawitihotmail.onmicrosoft.com] FROM EXTERNAL PROVIDER;
        GO


        -- ContaineDB
        CREATE USER [matar-test@maalhawitihotmail.onmicrosoft.com] FROM EXTERNAL PROVIDER;
        GO

        ALTER ROLE db_owner ADD MEMBER [matar-test@maalhawitihotmail.onmicrosoft.com];
        GO


# High availability with Azure Cosmos DB
    For high availability --> always configure your Cosmos accounts to have multiple write regions.
    t's always recommended to set up at least two regions (preferably, at least two write regions)